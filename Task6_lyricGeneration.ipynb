{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业第6周  歌词生成练习"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.仿照课件关于歌词生成例子，在课件示例基础上将LSTM网络改为GRU且多层堆砌，优化网络层数及其它参数，尽力提升效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n"
     ]
    }
   ],
   "source": [
    "#首先执行GPU资源分配代码，勿删除。\n",
    "import GPU\n",
    "# GPU.show()\n",
    "GPU.alloc(0,1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入需要的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import random \n",
    "import zipfile\n",
    "import numpy as np \n",
    "import math \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN,LSTM,GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "print(corpus_chars[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、建立字符索引\n",
    "我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。  \n",
    "为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。  \n",
    "接着，打印 vocab_size，即词典中不同字符的个数，又称词典大小。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用set()函数将数据集中重复的字符删掉，然后放入列表中。\n",
    "idx_to_char=list(set(corpus_chars))\n",
    "len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将字符映射到索引\n",
    "char_to_idx = {char:i for i, char in enumerate(idx_to_char)}\n",
    "vocab_size=len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63282"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将字符转化成索引\n",
    "corpus_indices= [char_to_idx[char] for char in corpus_chars]\n",
    "len(corpus_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus_indices 中是原数据集corpus_chars 中所有字符的索引值。我们可以打印前20个字符机器对应的索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [139, 1794, 1763, 475, 300, 2065, 139, 2264, 2259, 2264, 2259, 2252, 1950, 2399, 1366, 139, 976, 300, 752, 1383]\n",
      "chars: \n",
      "随风跟著我\n",
      "一口一口吃掉忧愁\n",
      "载著你 \n"
     ]
    }
   ],
   "source": [
    "sample=corpus_indices[1000:1020]\n",
    "print('indices:', sample)\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、时序数据的采样\n",
    "在训练中我们需要每次随机读取小批量样本和标签。与其他实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。  \n",
    "## 3.1 随机采样\n",
    "下面的代码每次从数据里随机采样一个小批量。  \n",
    "其中批量大小 batch_size 指每个小批量的样本数，num_steps为每个样本所包含的时间步数。   \n",
    "在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。 \n",
    "## 3.2 相邻采样\n",
    "除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面，在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。   \n",
    "为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[ 0  1  2  3  4  5]\n",
      " [15 16 17 18 19 20]] \n",
      "Y: [[ 1  2  3  4  5  6]\n",
      " [16 17 18 19 20 21]] \n",
      "\n",
      "X:  [[ 6  7  8  9 10 11]\n",
      " [21 22 23 24 25 26]] \n",
      "Y: [[ 7  8  9 10 11 12]\n",
      " [22 23 24 25 26 27]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices=np.array(corpus_indices)\n",
    "    data_len=len(corpus_indices)\n",
    "    batch_len=data_len//batch_size\n",
    "    indices=corpus_indices[0: batch_size*batch_len].reshape((batch_size, batch_len))\n",
    "    epoch_size= (batch_len-1) //num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i=i*num_steps\n",
    "        X=indices[:, i: i+num_steps]\n",
    "        Y=indices[:, i+1: i+num_steps+1]\n",
    "        yield X, Y\n",
    "        \n",
    "my_seq=list(range(30))\n",
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、定义模型\n",
    "## 4.1 定义循环神经网络层 \n",
    "Keras 的 Rnn 模块提供了循环神经网络的实现。下面构造一个含单隐藏层、隐藏单元个数为256的循环神经网络层 rnn_layer，并对权重做初始化。\n",
    "rnn_layer 的输入形状为 (时间步数, 批量大小, 词典大小)，在前向计算后会分别返回输出和隐藏状态。  \n",
    "其中输出指的是隐藏层在各个时间步上计算并输出的隐藏状态，它们通常作为后续输出层的输入。  \n",
    "需要强调的是，该“输出”本身并不涉及输出层计算，形状为 (时间步数, 批量大小, 隐藏单元个数)。  \n",
    "返回的隐藏状态指的是隐藏层在最后时间步的隐藏状态：当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (160, 35, 256)            661248    \n",
      "                                                                 \n",
      " gru (GRU)                   (160, 35, 256)            394752    \n",
      "                                                                 \n",
      " dense (Dense)               (160, 35, 2583)           663831    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,719,831\n",
      "Trainable params: 1,719,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_hiddens = 256\n",
    "batch_size = 160\n",
    "num_steps = 35\n",
    "model = Sequential()\n",
    "model.add(keras.Input(batch_input_shape = (batch_size,num_steps))) \n",
    "model.add(Embedding(output_dim = 256,input_dim = vocab_size, input_length = num_steps))\n",
    "#model.add(LSTM(units=num_hiddens,return_sequences=True, stateful=True))\n",
    "model.add(GRU(units = num_hiddens,return_sequences = True, stateful = True))\n",
    "model.add(Dense(units = vocab_size,activation = 'softmax' ))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This account is currently not available.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot graphviz\n",
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model = model,show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将经过采样后的样本直接输入此网络中时，它经过的处理操作依次为：  \n",
    "1. 输入形状为 (批量大小, 时间步数)  \n",
    "2. 利用embedding编码得到 (批量大小, 时间步数, embedding大小  \n",
    "3. 输入到 rnn 层，得到 (批量大小, 时间步数, 隐藏单元个数)\n",
    "4. 经过 Dense 层，得到 (批量大小， 时间步数, 词典大小)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature = 1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def predict_rnn_keras(prefix, num_chars):\n",
    "    # 使用model的成员函数来初始化隐藏状态\n",
    "    model.reset_states()\n",
    "    output= [char_to_idx[prefix[0]]]  #上一次输出\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = (np.array([output[-1]]).repeat(batch_size)).reshape((batch_size, 1))  #timestep=1\n",
    "        Y = model(X)  # 前向计算\n",
    "        if t < len(prefix) -1:\n",
    "            output.append(char_to_idx[prefix[t+1]])   #引导前缀不使用预测结果\n",
    "        else:\n",
    "            output.append(sample(np.array(Y[0,0,:])))  #批量大小， 时间步数, 词典大小\n",
    "            ##output.append(int(np.array(tf.argmax(Y,axis=-1))[0,0]))  #批量大小， 时间步数, 词典大小\n",
    "    return''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开榜喉陽剧犹鼠哼胖扇讨'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn_keras('分开', 10)\n",
    "#因为模型参数为随机值，所以预测结果也是随机的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、裁剪梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算裁剪后的梯度示意代码，本例并不使用该函数！！！\n",
    "def grad_clipping(grads,theta):\n",
    "    norm=np.array([0]) \n",
    "    for i in range(len(grads)):\n",
    "        norm+=tf.math.reduce_sum(grads[i] **2)\n",
    "    norm=np.sqrt(norm).item()\n",
    "    if norm <= theta:\n",
    "        return grads\n",
    "    \n",
    "    new_gradient=[]\n",
    "    for grad in grads:\n",
    "        new_gradient.append(grad*theta/norm)\n",
    "    return new_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6、定义模型训练函数\n",
    "初始化优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-3, clipnorm=0.1)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7训练\n",
    "定义梯度下降函数定义训练函数    \n",
    "num_epochs：训练次数；   \n",
    "batch_size：批次大小；   \n",
    "pred_period：间隔多少次展示一次结果；   \n",
    "pred_len：要求预测的字符长度。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, perplexity 2.367327\n",
      ">> 想要我看你不要\n",
      "这里你再也只想很简单\n",
      "不想再见\n",
      "训导处报告 快使用双截棍 \n",
      "生命中只是我和你融化了开\n",
      "只\n",
      ">> 我们忏悔\n",
      "我的世界流浪\n",
      "却又过去对医药箱说\n",
      "别怪我\n",
      "别怪我 屌  却想为了我要一点孤独\n",
      "力会让半魔之前\n",
      "\n",
      "epoch 200, perplexity 1.202386\n",
      ">> 想要有直升机\n",
      "想要打我在等动\n",
      "用第一脸 在想念失去\n",
      "你了把 长大的梦 的呦啊\n",
      "我把天地下的每个人\n",
      "都是我\n",
      ">> 我们忏悔\n",
      "生活习惯 这时候 不稀 \n",
      "我知道是自己往前旁\n",
      "木炭 剩一半\n",
      "雨过蝴蝶\n",
      "下大的国度性\n",
      "就像是多汁\n",
      "epoch 300, perplexity 1.068957\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你的笑\n",
      "一\n",
      ">> 我们忏悔\n",
      "生死不想太多\n",
      "会不会慢慢走开\n",
      "为什么我连分开都迁就着你\n",
      "我真的没有天份\n",
      "安静的没这么快\n",
      "我会学\n",
      "epoch 400, perplexity 1.035919\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天我遇见你的微笑很久以\n",
      "我在边功\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我只有一天的回忆\n",
      "雨淋湿了天空\n",
      "毁得很讲究\n",
      "你说你\n",
      "epoch 500, perplexity 1.024284\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和外婆参加  \n",
      "吹着残忍住大\n",
      "那有三光 训导处报告\n",
      "三年二班周杰伦\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "一起旅行\n",
      "那童年的希望是\n",
      "一台 时光机\n",
      "你我翻滚过的榻榻米\n",
      "味道\n",
      "epoch 600, perplexity 1.020293\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "这样的\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我们的爱 给的太快 \n",
      "是从此就分的\n",
      "让回忆皎洁\n",
      "爱\n",
      "epoch 700, perplexity 1.011737\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "这样的\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我们在地平线感谢\n",
      "爱停止向另一边倾斜\n",
      "这首部曲的管\n",
      "epoch 800, perplexity 1.011386\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "这样的\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我们在地平线感谢\n",
      "爱停止向另一边倾斜\n",
      "这首部曲的管\n",
      "epoch 900, perplexity 1.013672\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "这样的\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我们在地平线感谢\n",
      "爱停止向另一边倾斜\n",
      "这首部曲的管\n",
      "epoch 1000, perplexity 1.014976\n",
      ">> 想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "这样的\n",
      ">> 我们忏悔\n",
      "生命珍贵 我用那十四行诗赞美\n",
      "年轻的 一切\n",
      "我们在地平线感谢\n",
      "爱停止向另一边倾斜\n",
      "这首部曲的管\n"
     ]
    }
   ],
   "source": [
    "def train_and_predict_rnn_keras(num_epochs, batch_size, pred_period, pred_len, prefixes):\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n=0.0, 0\n",
    "        model.reset_states()\n",
    "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            y_pred = model.train_on_batch(X,Y)     #y_pred[0]为loss， y_pred[1]为accuracy\n",
    "            loss=y_pred[0]\n",
    "            l_sum+=loss\n",
    "            n+=1\n",
    "            \n",
    "        if (epoch+1) % pred_period==0:\n",
    "            print('epoch %d, perplexity %f'% (epoch+1, math.exp(l_sum/n)))\n",
    "            for prefix in prefixes:\n",
    "                print('>>', predict_rnn_keras(prefix, pred_len))\n",
    "                \n",
    "num_epochs = 1000\n",
    "train_and_predict_rnn_keras(num_epochs, batch_size, 100, 50, ['想要', '我们'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注解：困惑度我们通常使用困惑度（perplexity）来评价语言模型的好坏。**  \n",
    "困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；  \n",
    "最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；  \n",
    "基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。  \n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。  \n",
    "在本例中，困惑度必须小于词典大小vocab_size。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
